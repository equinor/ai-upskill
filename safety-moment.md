# Safety moment

_Matt Hall, TDI EDT DSD, `mtha@equinor.com`_

Think back to the last time you flew in a plane. Maybe it was earlier this morning.

How many affordances to safety can you see from your seat?

There are quite a few: smoke detectors, emergency alarms and lighting, escape route marking, the safety briefing, notices everywhere, flame retardent materials, even your seat can withstand 16G (about double what you can stand!). Of course, there is national and international legislation, construction codes, inspections, and all sorts of infrastructure keeping us safe.

Aviation is perfectly dangerous, yet it's the safest way to travel today... but of course it was not always like that.

The first flight was in 1903. The US did not seriously regulate the activity until the Air Commerce Act in 1926, and they only did that because the industry asked them to. The international community was a bit more organized and met in 1910 to start establishing a code of law. The Great War slowed things down a bit, but the International Civil Aviation Organization was established in 1922, nearly 20 years after the first flight, and at a time when you could probably fit everyone who had ever flown into a large room. 

ICAO alone publishes a lot of safety regulation, like the 65-page [Document 10049, Manual on the Approval and Use of Child Restraint Systems](https://d3n8a8pro7vhmx.cloudfront.net/afacwa/pages/2302/attachments/original/1532020664/10049_Manual_on_use_of_CRS_english_final.pdf?1532020664). They update and publish thousands of such manuals multiple times each year.

The arrival of generative AI, especially the release (or detonation?) of ChatGPT at the end of November 2022, was perhaps analogous to that first flight. A new but potentially dangerous technology. But ChatGPT reached 100 million users in less than 2 months. Flight took decades to reach that number of users, and even the World-Wide Web took 7 years. But per today, generative AI is essentially unregulated and &mdash; worse &mdash; society has almost no intuition for when or how or why it can be harmful.

Let's look at `chat.equinor.com`. Has anyone **never** used this tool? Feel free to follow along.

Let's ask a question: "Tell me about Equinor." The answer is quite reasonable. There is no doubt that this technology is remarkable.

I have a follow-up question, "Why did Statoil get out of the whale oil business in 1987?". The model will almost always reply with a completely made up story (Statoil was never in the whale oil business), revealing an error mode called **hallucination**. The justification can be quite elaborate. As usual, it is eloquent and very believable.

In my experience, the model tends to hallucinate when asked leading questions about things that are not true, or misleading questions about things that are. It's a fun group exercise to come up with one about Equinor or about your city.

Here are some others:

- `Can I drink molten ice?` (that is, water)
- `Why is Mercury the closest planet to Jupiter, on average?` (this is true for all of the planets)
- `What are Equinor's brand colours?`
- `Why did Buzz Aldrin bring a Buzz Lightyear toy on Apollo 11?`
- `Tell me something amazing about the recreational facilities on the Stavanger Field platform.`

## Resources

- Equinor's Responsible AI team is led by **Rialda Spahic**, based in Trondheim. Get in touch with them!
- If you're interested in learning more about the risks associated with LLMs and generative AI, [this video from Phaedra Boinodiris at IBM](https://www.youtube.com/watch?v=r4kButlDLUc) is informative and technical but totally approachable. IBM's other videos are also great.
- If you're looking for more education content or need training for your team, get in touch with [AI Upskill](https://statoilsrm.sharepoint.com/sites/DigitalAcademy2/SitePages/AI-Upskill.aspx).
